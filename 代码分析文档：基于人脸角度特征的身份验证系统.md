### 代码分析文档：基于人脸角度特征的身份验证系统

#### 1. 引言

本项目旨在实现一个基于 **FaceNet** 的人脸识别系统，利用来自 **ExtYaleB** 数据集的图像进行身份验证。该数据集包含了不同光照条件下的图像，其中每张图像的方位角（azimuth）和仰角（elevation）与光源的相对位置有关。我们的目标是通过提取图像的特征向量，结合光照角度对人脸进行分类，进而验证身份。

该项目的实现受到了论文 **"Discriminative Models for Face Recognition with Inconsistent Lighting"** 的启发。论文中的方法利用光照条件对图像进行分箱（binning）处理，并通过 **FaceNet** 模型进行特征提取，以便准确地从不同光照条件下进行身份识别。

#### 2. 数据预处理与光照角度提取

##### 2.1 文件读取与排序

首先，程序读取存放人脸图像的文件夹路径，并过滤掉含有 "Ambient" 的图像文件（这些通常是非面部图像）。接着，程序对剩余的图像文件按照文件名进行排序，并通过 `random.shuffle` 随机打乱文件顺序，这有助于去除文件排序带来的偏差：

```
files = glob.glob('.\CroppedYalePNG/*.png') 
files = [file_ for file_ in files if 'Ambient' not in os.path.basename(file_)] 
files = sorted(files)  
random.shuffle(files)
```

这种预处理步骤遵循了论文中的数据排序与打乱策略，用于减少因图像排序产生的系统偏差。

##### 2.2 方位角与仰角的提取

根据 **论文** 中的描述，我们的目标是提取图像的最大光照角度，并据此对数据进行分类。图像文件名中的方位角（azimuth）和仰角（elevation）被提取并用于计算每个图像的最大角度值：

```
def get_max_azim_elev(file_):
    base = os.path.basename(file_)
    base = os.path.splitext(base)[0]
    azim, elev = base.split('_')[-1].split('A')[-1].split('E')
    azim, elev = int(azim), int(elev)
    return max(abs(azim), abs(elev))

thetas = list(map(get_max_azim_elev, files))
```

此方法对应了论文中的 **光照角度计算** 过程，旨在通过方位角和仰角的绝对值计算得到每张图像的最大角度值。论文中提到，最终我们关心的是方位角和仰角的绝对值中的最大值，类似于代码中的 `max(abs(azim), abs(elev))`。

##### 2.3 光照角度的分布与可视化

根据 **论文** 中的方法，代码生成了光照角度分布的直方图，用以分析角度值的分布情况。这种方式能够有效地了解数据集中不同光照条件下的图像分布，帮助后续分类处理：

```
fig = plt.figure()
plt.hist(thetas, bins='auto', histtype='bar', ec='white')
plt.xlabel('max(abs(azim), abs(elev)) value')
plt.ylabel('Freq')
plt.title("Histogram of theta values across ExtYaleB")
plt.show()
```

直方图展示了角度值的分布情况，为后续的分类操作提供了直观的依据。

#### 3. 光照角度的分箱与分类

根据论文中的 **Sec 4.3**，我们将光照角度值分为多个类别（bins），用于分类不同的光照条件。与论文一致，代码将光照角度分为 5 个类别，并对每个角度值进行分类。这一过程与论文中提出的 **“光照分类”** 方法一致：

```
def bin_theta(theta):
    if theta < 12:
        return 0
    elif 20 <= theta <= 25:
        return 1
    elif 35 <= theta <= 50:
        return 2
    elif 60 <= theta < 77:
        return 3
    elif theta > 77:
        return 4
    else:
        raise ValueError

y = np.array(list(map(bin_theta, thetas)))
```

这些类别代表了不同强度的光照条件，其中类别 0 代表最强光照，类别 4 代表最弱光照。论文中指出，类别的划分有助于将光照条件相似的图像分为同一类，以提高后续识别过程的准确性。

#### 4. 面部特征提取与相似度计算

在特征提取阶段，代码使用 **FaceNet** 模型提取每张正面图像的特征向量。每个人的第一张正面图像会被用作该人的特征，后续的图像则用于验证身份。此方法与论文中所描述的 **面部特征提取** 一致：

```
embedder = FaceNet()

for i in range(1, 40):
    person_id = f'yaleB{i:02d}'
    for filename in os.listdir(data_dir):
        if filename.endswith('.png') and not filename.endswith('Ambient.png') and person_id in filename:
            image = cv2.imread(img_path)
            image_resized = cv2.resize(image, (160, 160))
            embeddings = embedder.embeddings(np.expand_dims(image_resized, axis=0))
            feature_vectors[person_id] = embeddings[0]
```

特征提取过程遵循论文中的方法，通过 **FaceNet** 模型将图像转化为向量空间中的点，进而计算其相似度。

#### 5. 验证与测试

在验证阶段，程序计算每张测试图像与训练图像的特征向量之间的欧几里得距离（L2 距离），并找到最相似的训练图像。如果测试图像与最相似的训练图像具有相同的标签，则认为身份验证成功。这一过程类似于论文中的 **基于特征相似度的身份验证** 方法：

```
distances = {ref_id: np.linalg.norm(embeddings[0] - ref_embedding) for ref_id, ref_embedding in feature_vectors.items()}
closest_id = min(distances, key=distances.get)
```

通过这种方式，系统能够准确地验证身份，并根据角度条件区分低角度与高角度图像的识别效果。

#### 6. 准确率评估

根据论文中的评估方法，程序计算了系统在低角度（`azimuth` 和 `elevation` 小于等于 45 度）和高角度图像上的识别准确率。代码分别计算了总体准确率、低角度准确率和高角度准确率：

```
accuracy = correct_count / total_count if total_count > 0 else 0
low_degree_accuracy = low_degree_correct_count / low_degree_total_count if low_degree_total_count > 0 else 0
```

这些评估结果展示了在不同光照条件下模型的表现。与论文中的结论一致，低角度图像的识别准确率通常较高，而高角度图像的准确率相对较低。

#### 7. 总结

本项目基于论文 **"Discriminative Models for Face Recognition with Inconsistent Lighting"** 的方法实现了一个基于光照角度的面部识别系统。通过利用 **FaceNet** 模型提取人脸特征，并结合光照角度对图像进行分类，我们成功地完成了对 **ExtYaleB** 数据集的身份验证任务。项目中遵循了论文中提出的数据预处理、特征提取和分类方法，并通过准确率评估验证了系统在不同光照条件下的表现。未来的研究可以进一步优化分类模型，提升高角度图像的识别准确率。